# LLM-based annotation

This directory contains the tools for LLM-based RFB annotation. The workflow is as follows:

1. - Get a batch of OCR annotations from video frames that you want to use for training data.
    - Run the batch through the `scenes-with-text` app to get scene labels, and only keep `credits` and `chyrons`
    - Using the streamlit annotation environment `review_ocr.py`, correct misclassifications and throw out OCR results that are too poor
2.  - Feed OCR results into a cloud-based LLM (Claude-3 Haiku was used for the current annotation batch) to get sequence-labelled "silver standard" annotations.
    - Using the streamlit adjudication environment `llm_adjudicator.py`, either accept, correct, or reject the LLM's annotations
3. Feed the corrected annotations to the RFB model

## Running the annotation environments

Both annotation environments can be run using `streamlit run <filename>`. Since the apps gather frames from AAPB GUIDs and timepoints, **they must be run from a lab server if you do not have the videos saved locally!** Running them from your local device will cause an error.

You can upload an annotation file from your local device to the streamlit uploader. Once you start annotating, a copy with all annotated changes will be stored in the llm-silver-anno directory on the server. If you need to come back and make later changes, you can use the same local file -- even if the local file is unannotated, it will reference the server copy and pick up from where you left off.

## Data format

Both `llm_adjudicator.py` and `review_ocr.py` expect input data in CSV format. Example files for both are stored in the `examples` directory. The apps expect files with the following columns:

### OCR Reviewer:
| Field | Description |
|-------|-------------|
| guid | AAPB GUID of the source video |
| timePoint | Frame number of the image |
| scene_label | SWT-assigned scene label |
| confidence | SWT confidence score |
| textdocument | The OCR content |
| path | Path to the source video |

#### LLM Adjudicator
The adjudication environment takes the output of the OCR reviewer annotations, plus:

- silver_standard_annotation
    - The LLM output in indexed BIO format (pseudo NER). It should be in this format: 
    
        `Barber@BF:1 Conable@IF:1 Host@BR:1`

        Where matching roles and fillers are co-indexed

So, the total columns expected are:

| Field | Description |
|-------|-------------|
| guid | (carried over) |
| timePoint | (carried over) |
| scene_label | (carried over) |
| confidence | (carried over) |
| textdocument | (carried over) |
| path | (carried over) |
| ocr_accepted | Whether the OCR was accepted |
| deleted | True if the OCR was a strange edge case or a non-accepted frame type |
| cleaned_text | OCR text cleaned by removing rows without alphabetical characters and discarding newlines. This is either user-generated by running `python utils/clean_ocr.py <input.csv> <ouput.csv>` or auto-generated when the adjudication environment is started |
| silver_standard_annotation | LLM BIO annotation |


> [!NOTE]  
> We found that Claude produces better annotations when fed cleaned text rather than raw textdocuments, so it may be better to run `utils/clean_ocr.py` on your data before passing it to your chosen LLM. However, the annotation environment will work even if cleaned_text is not provided, so this is by no means a requirement.

#### Output format
The adjudicator produces a CSV with all input values, and stores the output in two columns:

| Field | Description |
|-------|-------------|
| adjudicated | Whether the row has been processed |
| accepted | Whether the annotation was accepted. If false, it should be discarded from the data batch. (Note, this will also be set to true if the user manually corrected the LLM annotation) |